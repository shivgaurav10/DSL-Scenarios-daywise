data1 = [
    (1, "A", "A", 1000000),
    (2, "B", "A", 2500000),
    (3, "C", "G", 500000),
    (4, "D", "G", 800000),
    (5, "E", "W", 9000000),
    (6, "F", "W", 2000000),
]
df1 = spark.createDataFrame(data1, ["emp_id", "name", "dept_id", "salary"])
df1.show()

data2 = [("A", "AZURE"), ("G", "GCP"), ("W", "AWS")]
df2 = spark.createDataFrame(data2, ["dept_id1", "dept_name"])
df2.show()
#inner join df1 and df2 on id

result_df = df1.join(df2, df1["dept_id"] == df2["dept_id1"], "inner")
result_df.show()

###### Define window
from pyspark.sql.window import Window
specifywindow = Window.partitionBy("dept_id").orderBy(col("salary").desc())
###Apply window
densrank = result_df.withColumn("drank", dense_rank().over(specifywindow))
densrank.show()

#filter 1st rank only

frank = densrank.filter("drank = 2").drop("denserank")
finalresult=frank.select("emp_id","name","dept_id","salary")
finalresult.show()
